---
output:
  html_document: default
  word_document: default
---

## Set path and load library
```{r}
setwd("D:\\Data Science ToolBox\\PROJECT\\Data")

load.library <- c ("ggplot2", "dplyr","MASS", "corrplot","randomForest", "Matrix", "methods" ,"foreach", "adabag","gbm","xgboost")

sapply(load.library, require, character = TRUE)

```

## Read training set and test set

```{r}
connect_train <- "train.csv"

connect_test <- "test.csv"

train <- read.csv(connect_train, header = TRUE, sep = ",", stringsAsFactors = TRUE)
test <- read.csv(connect_test, header = TRUE, sep = ",", stringsAsFactors = TRUE)


```

## Structure of the data
```{r}
dim(train)

str(train)
```

## After that, the whole procedure has begun. I divide the whole process into four steps:

1. Data Cleansing
2. Descriptive Analysis
3. Model Selection
4. Final Prediction

# CLEANING DATA
### It is important to clean the data with some specific rules, otherwise the precision of result can be jeopardized. After summarizing training set, it is not diffcult to find that some data columns got too many missing values. We first have look on the number of missing values in every variable.

```{r}
colSums(sapply(train,is.na))

```

###  The percentage of data missing in train.

```{R}
sum(is.na(train))*100/ (nrow(train)*ncol(train))

```

### The percentage of data missing in test set.

```{r}
sum(is.na(test))/ (nrow(test)*ncol(test))

```

### Check for the number of duplicate rows

```{r}
cat ("The number of duplicate rows in training set are : ", nrow(train) - nrow(unique(train)))

```

### Among 1460 variables, 'Alley', 'PoolQC', 'Fence' and 'MiscFeature' have amazingly high number of missing value. Therefore, I have decided to remove those variables.
```{r}
train<- train[,-c(7,73,74,75)]

```

### For the remaining missing values, I replaced them with zero directly. The data cleansing procedure ends here

```{r}
train[is.na(train)] <- 0

```

#### Transfer categorical features to integer/numerical features 

```{r}

num <- sapply(train, is.numeric)

num <- train[num]

for(i in 1:77){
  if(is.factor(train[,i])){
    train[,i]<-as.integer(train[,i])
  }
}

train <- train[,-1]
train[is.na(train)] <- 0


```

# DATA VISUALIZATION
### We first draw a corrplot of numeric variables. Those with strong correlation with sale price are examined.

```{r}
correlations <- cor (num[,-1], use = "everything")
corrplot(correlations, method = "circle", type = "lower", sig.level = 0.01, insig = "p-value")

```
### 'OverallQual','TotalBsmtSF','GarageCars','GrLivArea' and 'GarageArea' have relative strong correlation with each other. Therefore, as an example, we plot the correlation among those fives variables and SalePrice.

```{r}
pairs(~SalePrice+OverallQual+TotalBsmtSF+GarageCars+GarageArea + GrLivArea,data=num,
      main="Scatterplot Matrix")

```

### (SalePrice) looks having decent linearity when plotting with other variables. However, it is also obvious that some independent variables also have linear relationship with others. The problem of multicollinearity is obvious and should be treated when the quantity of variables in regression formula is huge.

### The final descriptive analysis I put here would be the relationship between the variable 'YearBu' and Sale Price.

```{r}
p<- ggplot(num,aes(x= YearBuilt,y=SalePrice))+geom_point()+geom_smooth()

print (p)

```

### It is not difficult to see that SalePrice of the house increases generally with the year built, the trend is obvious.

# MODEL SELECTION

### Before implementing models, one should first split the training set of data into 2 parts: a training set within the training set and a test set that can be used for evaluation. Personally i prefer to split it with ratio 5:3. 

```{r}
positions <- sample(1:nrow(train),size = floor(nrow(train)/5)*3)

training <- train[positions,]
testing <- train[-positions,]
```

### I will fit three regression models to the training set and choose the most suitable one by checking RMSE value

## Model 1: Linear Regression
### The first and simplest but useful model is linear regression model. As the first step, I put all variables into the model.

```{r}

reg <- lm(SalePrice ~., data = training)

summary(reg)

```

### R Square is not bad, but many variables do not pass the Hypothesis Testing, so the model is not perfect. Potential overfitting will occur if someone insist on using it. Therefore, the variable selection process should be involved in model construction.
### Several variables still should not be involved in model. By checking the result of Hypothesis Test, I mannually build the final linear regression model.

```{r}
reg_modify <- lm(SalePrice ~ MSSubClass + LotFrontage + LotArea+ OverallQual + OverallCond + YearBuilt + MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + X1stFlrSF + X2ndFlrSF + BedroomAbvGr + GarageYrBlt + GarageCars + GarageArea + WoodDeckSF + ScreenPorch, data = training)

summary ( reg_modify)

```

### The R Square is not bad, and all variables pass the Hypothesis Test. The diagonsis of residuals is also not bad. The diagnosis can be viewed below.

```{r}
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(reg_modify)

```

### We check the performance of linear regression model with RMSE (Root mean square error)value.

```{r}

predictions <- predict(reg_modify, newdata = testing)

x <- predictions
y <- testing$SalePrice
plot(x,y,pch = 18,cex = 0.7, main="Predictions and real", col = "red")
sqrt((sum((testing$SalePrice-predictions)^2))/nrow(testing))

```

## Model 2:  Random Forest 
### The other model I chose to fit in the training set is Random Forest model. The model, prediction and RMSE calculation can be found below:

```{r}

ranfor <- randomForest(SalePrice~., data = training)
predictions <- predict(ranfor, newdata = testing)
sqrt(sum((testing$SalePrice - predictions)^2)/nrow(testing))

```
### Obviously, Random Forest may produce the best result within the training set so far.

## Model 3: Bagging
```{r}
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
registerDoParallel(cl)
outcomeName <- "SalePrice"
predictorNames <- setdiff(names(training), outcomeName)
bagging <- function (training, length_divisor = 4, iterations = 100){
predictions <- foreach(m = 1: iterations, .combine = cbind)%do%{
  training_positions <- sample(nrow(training), size = floor((nrow(training)/length_divisor)))
  train_pos <- 1:nrow(training) %in% training_positions
  ranfor_fit <- randomForest(SalePrice~., data = training[train_pos,])
  predict(ranfor_fit, newdata = testing[,predictorNames])
}
rowMeans(predictions)
}

predictions = bagging(training, 4, 100)
stopCluster(cl)
sqrt(sum((testing$SalePrice - predictions)^2)/nrow(testing))


```

# Model 4: Gradient Boosting

```{r}

modelgbm <-gbm(SalePrice~., data = training,n.trees = 10000, n.minobsinnode = 30, shrinkage = 0.01, distribution = "gaussian",cv.folds = 5, interaction.depth = 8)

n.trees = seq(from = 100, to = 10000, by = 100)
predmatrix<-predict(modelgbm,testing,n.trees = n.trees)
test.error<-with(testing,apply( (predmatrix-SalePrice)^2,2,mean))
head(test.error)
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")

predictions <- predict(modelgbm, newdata = testing,n.trees = gbm.perf(modelgbm, plot.it = FALSE))
sqrt(sum((testing$SalePrice - predictions)^2)/nrow(testing))
```
# Model 5: XGBOOST 
```{r}
training <- as.matrix(training,rownames.force = NA)
testing <- as.matrix(testing,rownames.force = NA)
train_data <- xgb.DMatrix(data= training, label = training[,"SalePrice"])
test_data <- xgb.DMatrix(data = testing)


All_rmse<- c()
Param_group<-c()
for (iter in 1:20) {
  param <- list(objective = "reg:linear",
                eval_metric = "rmse",
                booster = "gbtree",
                max_depth = sample(6:10, 1),
                eta = runif(1, 0.01, 0.3),
                gamma = runif(1, 0.0, 0.2), 
                subsample = runif(1, 0.6, 0.9),
                colsample_bytree = runif(1, 0.5, 0.8)
                
  )
  cv.nround = 500
  cv.nfold = 4
  mdcv <- xgb.cv(data=train_data, params = param, nthread=6, 
                 nfold=cv.nfold, nrounds=cv.nround,verbose = TRUE)
 # Least Mean_Test_RMSE as Indicator # 
  min_rmse<- min(mdcv$evaluation_log[,test_rmse_mean])
  All_rmse<-append(All_rmse,min_rmse)
  Param_group<-append(Param_group,param)
  # Select Param
  param<-Param_group[(which.min(All_rmse)*8+1):(which.min(All_rmse)*8+8)]
}

param <- list (
  objective = "reg:linear",
  eval_metric = "rmse",
  booster  = "gbtree",
  max_depth = 8,
  eta = 0.081,
  gamma = 0.18,
  subsample = 0.82,
  colsample_bytree = 0.654
)
  
modelxgb <- xgb.train(params = param,data = train_data,nrounds = 1000,
                      watchlist = list(train = train_data),verbose = TRUE,
                      print_every_n = 50, nthread = 6)
predxgb <- predict(modelxgb,testing)
sqrt(sum((testing[,"SalePrice"] - predxgb)^2)/nrow(testing))
```


# OR we can use caret package as following:
```{r}
cv.ctrl <- trainControl(method = "repeatedcv", repeats = 1,number = 3)
xgb.grid <- expand.grid(nrounds = 500,
                        max_depth = seq(6,10),
                        eta = c(0.01,0.3, 1),
                        gamma = c(0.0, 0.2, 1),
                        colsample_bytree = c(0.5,0.8, 1),
                        min_child_weight = 1,
                        subsample = 1
)
xgb_tune <-train(SalePrice ~.,
                 data=training,
                 method="xgbTree",
                 metric = "RMSE",
                 trControl=cv.ctrl,
                 tuneGrid=xgb.grid
)
predxgb <- predict(xgb_tune,testing)
sqrt(sum((testing[,"SalePrice"] - predxgb)^2)/nrow(testing))

```

# As you can see that, Xgboost is doing so good in our data. Now, i decide to predict on test data
```{r}
test<- test[,-c(7,73,74,75)]
for(i in 1:76){
  if(is.factor(test[,i])){
    test[,i]<-as.integer(test[,i])
  }
}
test[is.na(test)]<-0

train <- as.matrix(train,rownames.force = NA)
test <- as.matrix(test,rownames.force = NA)
train_data <- xgb.DMatrix(data= train, label = train[,"SalePrice"])
test_data <- xgb.DMatrix(data = test)

param <- list (
  objective = "reg:linear",
  eval_metric = "rmse",
  booster  = "gbtree",
  max_depth = 8,
  eta = 0.081,
  gamma = 0.18,
  subsample = 0.82,
  colsample_bytree = 0.654
)

modelxgb <- xgb.train(params = param,data = train_data,nrounds = 1000,
                      watchlist = list(train = train_data),verbose = TRUE,
                      print_every_n = 50, nthread = 6)
predxgb <- predict(modelxgb,testing)
```
