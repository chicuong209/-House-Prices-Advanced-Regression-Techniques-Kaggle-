---
output:
  html_document: default
  word_document: default
---

## Set path and load library
```{r}
setwd("D:\\Data Science ToolBox\\PROJECT\\Data")

load.library <- c ("ggplot2", "dplyr","MASS", "Metrics", "corrplot","randomForest", "Matrix", "methods" ,"foreach", "adabag","gbm","xgboost")

sapply(load.library, require, character = TRUE)

```

## Read training set and test set

```{r}
connect_train <- "train.csv"

connect_test <- "test.csv"

train <- read.csv(connect_train, header = TRUE, sep = ",", stringsAsFactors = FALSE)
test <- read.csv(connect_test, header = TRUE, sep = ",", stringsAsFactors = FALSE)


```

## Structure of the data
```{r}
dim(train)

str(train)
```

## After that, the whole procedure has begun. I divide the whole process into four steps:

1. Data Cleansing
2. Descriptive Analysis
3. Model Selection
4. Final Prediction

# CLEANING DATA
### It is important to clean the data with some specific rules, otherwise the precision of result can be jeopardized. After summarizing training set, it is not diffcult to find that some data columns got too many missing values. We first have look on the number of missing values in every variable.

```{r}
colSums(sapply(train,is.na))

```

###  The percentage of data missing in train.

```{R}
sum(is.na(train))*100/ (nrow(train)*ncol(train))

```

### The percentage of data missing in test set.

```{r}
sum(is.na(test))/ (nrow(test)*ncol(test))

```

### Check for the number of duplicate rows

```{r}
cat ("The number of duplicate rows in training set are : ", nrow(train) - nrow(unique(train)))

```

### Among 1460 variables, 'Alley', 'PoolQC', 'Fence' and 'MiscFeature' have amazingly high number of missing value. Therefore, I have decided to remove those variables. After that, the number of effective variables has shrunken to 75 (excluding id).

```{r}
train<- train[,-c(7,73,74,75)]

```

### For the remaining missing values, I replaced them with zero directly. The data cleansing procedure ends here

```{r}
train[is.na(train)] <- 0

```

#### Then, I transferred dummny variables into numeric form. Due to the intimidating size of dummy variables, I decided to transfer them directly by implementing 'as.integer' method. This is why I let the string as factor when reading the data file. The numeric variables are sorted out in particular for the convenience of descriptive analysis.

```{r}

num <- sapply(train, is.numeric)

train <- train[num]

train <- train[,-1]

```

# DATA VISUALIZATION

### Exploring dataset could be diffcult when the quantity of variables is quite huge. Therefore, I mainly focused on the exploration of numeric variables in this report. The descriptive analysis of dummy variables are mostly finished by drawing box plots. Some dummy variables, like 'Street', are appeared to be ineffective due to the extreme box plot. The numeric variables are sorted out before turning dummy variables into numeric form.

### We first draw a corrplot of numeric variables. Those with strong correlation with sale price are examined.

```{r}
correlations <- cor (train, use = "everything")
corrplot(correlations, method = "circle", type = "lower", sig.level = 0.01, insig = "p-value")

```
### 'OverallQual','TotalBsmtSF','GarageCars','GrLivArea' and 'GarageArea' have relative strong correlation with each other. Therefore, as an example, we plot the correlation among those fives variables and SalePrice.

```{r}
pairs(~SalePrice+OverallQual+TotalBsmtSF+GarageCars+GarageArea + GrLivArea,data=train,
      main="Scatterplot Matrix")

```

### (SalePrice) looks having decent linearity when plotting with other variables. However, it is also obvious that some independent variables also have linear relationship with others. The problem of multicollinearity is obvious and should be treated when the quantity of variables in regression formula is huge.

### The final descriptive analysis I put here would be the relationship between the variable 'YearBu' and Sale Price.

```{r}
p<- ggplot(train,aes(x= YearBuilt,y=SalePrice))+geom_point()+geom_smooth()

print (p)

```

### It is not difficult to see that SalePrice of the house increases generally with the year built, the trend is obvious.

### The workload of data exploration is huge so I decide to end it at here. More details can be digged out by performing descriptive analysis.

# MODEL SELECTION

### Before implementing models, one should first split the training set of data into 2 parts: a training set within the training set and a test set that can be used for evaluation. Personally i prefer to split it with ratio 6:4. 

```{r}
positions <- sample(1:nrow(train),size = floor(nrow(train)/5)*3)

training <- train[positions,]
testing <- train[-positions,]
```

### I will fit three regression models to the training set and choose the most suitable one by checking RMSE value

## Model 1: Linear Regression
### The first and simplest but useful model is linear regression model. As the first step, I put all variables into the model.

```{r}

reg <- lm(SalePrice ~., data = training)

summary(reg)

```

### R Square is not bad, but many variables do not pass the Hypothesis Testing, so the model is not perfect. Potential overfitting will occur if someone insist on using it. Therefore, the variable selection process should be involved in model construction. I prefer to use Step AIC method.

### Several variables still should not be involved in model. By checking the result of Hypothesis Test, I mannually build the final linear regression model.

```{r}
reg_modify <- lm(SalePrice ~ MSSubClass + LotFrontage + LotArea+ OverallQual + OverallCond + YearBuilt + MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + X1stFlrSF + X2ndFlrSF + BedroomAbvGr + GarageYrBlt + GarageCars + PoolArea, data = training)

summary ( reg_modify)

```

### The R Square is not bad, and all variables pass the Hypothesis Test. The diagonsis of residuals is also not bad. The diagnosis can be viewed below.

```{r}
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(reg_modify)

```

### We check the performance of linear regression model with RMSE (Root mean square error)value.

```{r}

predictions <- predict(reg_modify, newdata = testing)

x <- predictions
y <- testing$SalePrice
plot(x,y,pch = 18,cex = 0.7, main="Predictions and real", col = "red")
sqrt((sum((testing$SalePrice-predictions)^2))/nrow(testing))

```

## Model 2:  Random Forest 
### The other model I chose to fit in the training set is Random Forest model. The model, prediction and RMSE calculation can be found below:

```{r}

ranfor <- randomForest(SalePrice~., data = training)
predictions <- predict(ranfor, newdata = testing)
sqrt(sum((testing$SalePrice - predictions)^2)/nrow(testing))

```
### Obviously, Random Forest may produce the best result within the training set so far.

## Model 3: Bagging
```{r}
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
registerDoParallel(cl)
outcomeName <- "SalePrice"
predictorNames <- setdiff(names(training), outcomeName)
bagging <- function (training, length_divisor = 4, iterations = 100){
predictions <- foreach(m = 1: iterations, .combine = cbind)%do%{
  training_positions <- sample(nrow(training), size = floor((nrow(training)/length_divisor)))
  train_pos <- 1:nrow(training) %in% training_positions
  ranfor_fit <- randomForest(SalePrice~., data = training[train_pos,])
  predict(ranfor_fit, newdata = testing[,predictorNames])
}
rowMeans(predictions)
}

predictions = bagging(training, 4, 100)
stopCluster(cl)
sqrt(sum((testing$SalePrice - predictions)^2)/nrow(testing))


```

# Model 4: Gradient Boosting

```{r}
modelgbm <-gbm(SalePrice~., data = training,n.trees = 1000, n.minobsinnode = 30, shrinkage = 0.01, distribution = "gaussian",cv.folds = 5)
predictions <- predict(modelgbm, newdata = testing,n.trees = gbm.perf(modelgbm, plot.it = FALSE))
sqrt(sum((testing$SalePrice - predictions)^2)/nrow(testing))
```
# Model 5: XGBOOST 



